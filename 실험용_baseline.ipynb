{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af8a2c0-45fe-4d13-bebd-0dca87a7b71f",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c611c8-2226-433c-bf5f-343cc0b094af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리를 임포트합니다.\n",
    "import os\n",
    "from typing import Tuple, Callable, Union, List\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
    "import albumentations as A\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import wandb\n",
    "from wandb import Image as WandbImage\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f69e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 시드 설정 함수\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 시드 값 설정\n",
    "seed = 42\n",
    "set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d69e6a-a719-4a97-92ca-6354c873313f",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56f97229-e29f-479d-abab-0db8219d1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        root_dir: str, \n",
    "        info_df: pd.DataFrame, \n",
    "        transform: Callable,\n",
    "        is_inference: bool = False\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_inference = is_inference\n",
    "        self.image_paths = info_df['image_path'].tolist()\n",
    "        \n",
    "        if not self.is_inference:\n",
    "            self.targets = info_df['target'].tolist()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Union[Tuple[torch.Tensor, int], torch.Tensor]:\n",
    "        img_path = os.path.join(self.root_dir, self.image_paths[index])\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        if self.is_inference:\n",
    "            return image\n",
    "        else:\n",
    "            target = self.targets[index]\n",
    "            return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07d2d0-9585-45ce-8ece-4f69b98f6dd4",
   "metadata": {},
   "source": [
    "# Transform Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b1855c1-cf13-476d-aabd-d78e9e082ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchvisionTransform:\n",
    "    def __init__(self, is_train: bool = True):\n",
    "        common_transforms = [\n",
    "            transforms.Resize((448, 448)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "        \n",
    "        if is_train:\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    # AutoAugment(policy=AutoAugmentPolicy.IMAGENET),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "                ] + common_transforms\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transforms.Compose(common_transforms)\n",
    "\n",
    "    def __call__(self, image: Image.Image) -> torch.Tensor:\n",
    "        return self.transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a683988-0f73-4e43-907b-0d5209550abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsTransform:\n",
    "    def __init__(self, is_train: bool = True):\n",
    "        # 공통 변환 설정: 이미지 리사이즈, 정규화, 텐서 변환\n",
    "        common_transforms = [\n",
    "            A.Resize(448, 448),  # 이미지를 224x224 크기로 리사이즈\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 정규화\n",
    "            ToTensorV2()  # albumentations에서 제공하는 PyTorch 텐서 변환\n",
    "        ]\n",
    "        \n",
    "        if is_train:\n",
    "            # 훈련용 변환: 랜덤 수평 뒤집기, 랜덤 회전, 랜덤 밝기 및 대비 조정 추가\n",
    "            self.transform = A.Compose(\n",
    "                [\n",
    "                    A.HorizontalFlip(p=0.5),  # 50% 확률로 이미지를 수평 뒤집기\n",
    "                    A.Rotate(limit=15),  # 최대 15도 회전\n",
    "                    A.RandomBrightnessContrast(p=0.2),  # 밝기 및 대비 무작위 조정\n",
    "                ] + common_transforms\n",
    "            )\n",
    "        else:\n",
    "            # 검증/테스트용 변환: 공통 변환만 적용\n",
    "            self.transform = A.Compose(common_transforms)\n",
    "\n",
    "    def __call__(self, image) -> torch.Tensor:\n",
    "        # 이미지가 NumPy 배열인지 확인\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            raise TypeError(\"Image should be a NumPy array (OpenCV format).\")\n",
    "        \n",
    "        # 이미지에 변환 적용 및 결과 반환\n",
    "        transformed = self.transform(image=image)  # 이미지에 설정된 변환을 적용\n",
    "        \n",
    "        return transformed['image']  # 변환된 이미지의 텐서를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82f3416-86f2-430f-9260-d23904e757e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformSelector:\n",
    "    \"\"\"\n",
    "    이미지 변환 라이브러리를 선택하기 위한 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(self, transform_type: str):\n",
    "\n",
    "        # 지원하는 변환 라이브러리인지 확인\n",
    "        if transform_type in [\"torchvision\", \"albumentations\"]:\n",
    "            self.transform_type = transform_type\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown transformation library specified.\")\n",
    "\n",
    "    def get_transform(self, is_train: bool):\n",
    "        \n",
    "        # 선택된 라이브러리에 따라 적절한 변환 객체를 생성\n",
    "        if self.transform_type == 'torchvision':\n",
    "            transform = TorchvisionTransform(is_train=is_train)\n",
    "        \n",
    "        elif self.transform_type == 'albumentations':\n",
    "            transform = AlbumentationsTransform(is_train=is_train)\n",
    "        \n",
    "        return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938bcb2-9257-49cb-8d05-dd4a7bb25665",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f16fb24a-8d34-4ed6-8a33-2d153d12d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    간단한 CNN 아키텍처를 정의하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # 순전파 함수 정의\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f91493ca-c5c2-4950-916a-cc4304c7ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchvisionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Torchvision에서 제공하는 사전 훈련된 모델을 사용하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        num_classes: int, \n",
    "        pretrained: bool\n",
    "    ):\n",
    "        super(TorchvisionModel, self).__init__()\n",
    "        self.model = models.__dict__[model_name](pretrained=pretrained)\n",
    "        \n",
    "        # 모델의 최종 분류기 부분을 사용자 정의 클래스 수에 맞게 조정\n",
    "        if 'fc' in dir(self.model):\n",
    "            num_ftrs = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "        elif 'classifier' in dir(self.model):\n",
    "            num_ftrs = self.model.classifier[-1].in_features\n",
    "            self.model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f28c8e4f-a914-4b12-982e-d4a58863c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Timm 라이브러리를 사용하여 다양한 사전 훈련된 모델을 제공하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        num_classes: int, \n",
    "        pretrained: bool\n",
    "    ):\n",
    "        super(TimmModel, self).__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, \n",
    "            pretrained=pretrained, \n",
    "            num_classes=num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f2da081-9010-431d-a049-835d7bbea4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelector:\n",
    "    \"\"\"\n",
    "    사용할 모델 유형을 선택하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_type: str, \n",
    "        num_classes: int, \n",
    "        **kwargs\n",
    "    ):\n",
    "        \n",
    "        # 모델 유형에 따라 적절한 모델 객체를 생성\n",
    "        if model_type == 'simple':\n",
    "            self.model = SimpleCNN(num_classes=num_classes)\n",
    "        \n",
    "        elif model_type == 'torchvision':\n",
    "            self.model = TorchvisionModel(num_classes=num_classes, **kwargs)\n",
    "        \n",
    "        elif model_type == 'timm':\n",
    "            self.model = TimmModel(num_classes=num_classes, **kwargs)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown model type specified.\")\n",
    "\n",
    "    def get_model(self) -> nn.Module:\n",
    "\n",
    "        # 생성된 모델 객체 반환\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2977c7b-bc39-48f7-8155-ef6b6a03d6f8",
   "metadata": {},
   "source": [
    "# Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97471eb3-a979-4fb3-b976-6c3177c79f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    모델의 손실함수를 계산하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Loss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        outputs: torch.Tensor, \n",
    "        targets: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "        return self.loss_fn(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e3d21-5ab8-41b0-aa5c-e62ace8dc6a6",
   "metadata": {},
   "source": [
    "# Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "645626ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): 개선이 없을 때 몇 에포크를 기다릴지\n",
    "            min_delta (float): 성능이 개선되었다고 판단하는 최소 변화량\n",
    "        \"\"\"\n",
    "        self.patience = patience  # 개선되지 않아도 기다리는 최대 에포크 수\n",
    "        self.min_delta = min_delta  # 성능 개선이 없다고 판단하는 최소 변화량\n",
    "        self.counter = 0  # 개선되지 않은 에포크 수 카운트\n",
    "        self.best_loss = None  # 검증 손실의 최저값\n",
    "        self.early_stop = False  # 중지 플래그\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss  # 첫 번째 에포크의 손실 저장\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss  # 손실이 개선되면 갱신\n",
    "            self.counter = 0  # 카운터 초기화\n",
    "        else:\n",
    "            self.counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True  # patience를 초과하면 학습 중지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a90c673-6672-4066-a9ec-9975d7842be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module, \n",
    "        device: torch.device, \n",
    "        train_loader: DataLoader, \n",
    "        val_loader: DataLoader, \n",
    "        optimizer: optim.Optimizer,\n",
    "        scheduler: optim.lr_scheduler._LRScheduler,\n",
    "        loss_fn: torch.nn.modules.loss._Loss, \n",
    "        epochs: int,\n",
    "        result_path: str,\n",
    "        num_classes: int,  # num_classes 추가\n",
    "        patience: int = 5,  # Early Stopping patience 추가\n",
    "        min_delta: float = 0.01  # Early Stopping min_delta 추가\n",
    "    ):\n",
    "        # 클래스 초기화: 모델, 디바이스, 데이터 로더 등 설정\n",
    "        self.model = model  # 훈련할 모델\n",
    "        self.device = device  # 연산을 수행할 디바이스 (CPU or GPU)\n",
    "        self.train_loader = train_loader  # 훈련 데이터 로더\n",
    "        self.val_loader = val_loader  # 검증 데이터 로더\n",
    "        self.optimizer = optimizer  # 최적화 알고리즘\n",
    "        self.scheduler = scheduler  # 학습률 스케줄러\n",
    "        self.loss_fn = loss_fn  # 손실 함수\n",
    "        self.epochs = epochs  # 총 훈련 에폭 수\n",
    "        self.result_path = result_path  # 모델 저장 경로\n",
    "        self.best_models = []  # 가장 좋은 상위 3개 모델의 정보를 저장할 리스트\n",
    "        self.lowest_loss = float('inf')  # 가장 낮은 Loss를 저장할 변수\n",
    "        self.early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)  # EarlyStopping 초기화\n",
    "        self.num_classes = num_classes  # num_classes 추가\n",
    "        # global_step 변수 초기화\n",
    "        self.global_step = 0  # 배치 수를 추적하기 위한 변수\n",
    "        # wandb watch\n",
    "        wandb.watch(self.model, log='all')\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        # 모델 저장 경로 설정\n",
    "        os.makedirs(self.result_path, exist_ok=True)\n",
    "\n",
    "        # 현재 에폭 모델 저장\n",
    "        current_model_path = os.path.join(self.result_path, f'model_epoch_{epoch}_loss_{loss:.4f}.pt')\n",
    "        torch.save(self.model.state_dict(), current_model_path)\n",
    "\n",
    "        # 최상위 3개 모델 관리\n",
    "        self.best_models.append((loss, epoch, current_model_path))\n",
    "        self.best_models.sort()\n",
    "        if len(self.best_models) > 3:\n",
    "            _, _, path_to_remove = self.best_models.pop(-1)  # 가장 높은 손실 모델 삭제\n",
    "            if os.path.exists(path_to_remove):\n",
    "                os.remove(path_to_remove)\n",
    "\n",
    "        # 가장 낮은 손실의 모델 저장\n",
    "        if loss < self.lowest_loss:\n",
    "            self.lowest_loss = loss\n",
    "            best_model_path = os.path.join(self.result_path, 'best_model.pt')\n",
    "            torch.save(self.model.state_dict(), best_model_path)\n",
    "            print(f\"Save {epoch}epoch result. Loss = {loss:.4f}\")\n",
    "\n",
    "    def train_epoch(self, epoch) -> tuple:\n",
    "        # 한 에폭 동안의 훈련을 진행\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        progress_bar = tqdm(self.train_loader, desc=\"Training\", leave=False)\n",
    "        scaler = GradScaler()  # AMP를 위한 GradScaler 객체 생성\n",
    "\n",
    "        for images, targets in progress_bar:\n",
    "            images, targets = images.to(self.device), targets.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # autocast 컨텍스트 내에서 모델을 실행하여 정밀도를 관리\n",
    "            with autocast():\n",
    "                outputs = self.model(images)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "\n",
    "            # 스케일링된 손실을 사용하여 역전파 실행\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # 스케일러를 사용해 가중치를 업데이트\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            # wandb에 배치별 손실 및 정확도 로깅\n",
    "            self.global_step += 1\n",
    "            batch_accuracy = 100.0 * predicted.eq(targets).sum().item() / targets.size(0)\n",
    "            wandb.log({\n",
    "                'train_loss_batch': loss.item(),\n",
    "                'train_accuracy_batch': batch_accuracy,\n",
    "                'global_step': self.global_step\n",
    "            })\n",
    "\n",
    "        # 전체 정확도 계산\n",
    "        train_accuracy = 100.0 * correct / total\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            'train_loss_epoch': total_loss / len(self.train_loader),\n",
    "            'train_accuracy_epoch': train_accuracy,\n",
    "            'epoch': epoch+1\n",
    "        })\n",
    "\n",
    "        return total_loss / len(self.train_loader), train_accuracy\n",
    "\n",
    "    def validate(self, epoch) -> tuple:\n",
    "        # 모델의 검증을 진행\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        misclassified_images = []\n",
    "        class_total = [0] * self.num_classes\n",
    "        class_correct = [0] * self.num_classes\n",
    "        class_misclassified = [0] * self.num_classes  # 클래스별 오분류 개수\n",
    "        progress_bar = tqdm(self.val_loader, desc=\"Validating\", leave=False)\n",
    "        \n",
    "        max_images_to_log = 20  # 로그할 최대 이미지 수\n",
    "        num_logged_images = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in progress_bar:\n",
    "                images, targets = images.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # 정확도 계산\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct_tensor = predicted.eq(targets)\n",
    "                correct += correct_tensor.sum().item()\n",
    "                total += targets.size(0)\n",
    "\n",
    "                true_labels.extend(targets.cpu().numpy())\n",
    "                pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "                # wandb에 배치별 검증 손실 로깅\n",
    "                wandb.log({\n",
    "                    'val_loss_batch': loss.item(),\n",
    "                    'global_step': self.global_step\n",
    "                })\n",
    "\n",
    "                # 클래스별 정답 및 오답 개수 집계\n",
    "                for t, p in zip(targets.view(-1), predicted.view(-1)):\n",
    "                    class_total[t.item()] += 1\n",
    "                    if t == p:\n",
    "                        class_correct[t.item()] += 1\n",
    "                    else:\n",
    "                        class_misclassified[t.item()] += 1\n",
    "\n",
    "                # 잘못 분류된 이미지 추출\n",
    "                mis_indices = (predicted != targets).nonzero(as_tuple=False).squeeze()\n",
    "                if mis_indices.numel() > 0 and num_logged_images < max_images_to_log:\n",
    "                    mis_images = images[mis_indices]\n",
    "                    mis_targets = targets[mis_indices]\n",
    "                    mis_preds = predicted[mis_indices]\n",
    "\n",
    "                    # 이미지 로그 (최대 max_images_to_log개)\n",
    "                    for img, true_label, pred_label in zip(mis_images, mis_targets, mis_preds):\n",
    "                        if num_logged_images >= max_images_to_log:\n",
    "                            break\n",
    "                        img = img.cpu()\n",
    "                        img_np = img.permute(1, 2, 0).numpy()\n",
    "                        # 이미지 정규화 해제\n",
    "                        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "                        img_np = np.clip(img_np, 0, 1)\n",
    "                        caption = f\"True: {true_label.item()}, Pred: {pred_label.item()}\"\n",
    "                        misclassified_images.append(wandb.Image(img_np, caption=caption))\n",
    "                        num_logged_images += 1\n",
    "\n",
    "        # 잘못 분류된 이미지들을 Wandb에 로그\n",
    "        wandb.log({\"misclassified_images\": misclassified_images, 'epoch': epoch+1})\n",
    "\n",
    "        # 전체 정확도 계산\n",
    "        val_accuracy = 100.0 * correct / total\n",
    "\n",
    "        # 클래스별 정확도 및 오분류 수 계산\n",
    "        per_class_accuracy = {}\n",
    "        per_class_misclassifications = {}\n",
    "        for i in range(self.num_classes):\n",
    "            if class_total[i] > 0:\n",
    "                accuracy = 100.0 * class_correct[i] / class_total[i]\n",
    "                misclassifications = class_misclassified[i]\n",
    "            else:\n",
    "                accuracy = None\n",
    "                misclassifications = None\n",
    "            per_class_accuracy[i] = accuracy\n",
    "            per_class_misclassifications[i] = misclassifications\n",
    "\n",
    "        # 클래스별 오분류 수를 wandb에 로그\n",
    "        wandb.log({\n",
    "            'per_class_misclassifications': wandb.Histogram(list(per_class_misclassifications.values())),\n",
    "            'epoch': epoch+1\n",
    "        })\n",
    "\n",
    "        # 가장 많이 틀린 클래스 상위 5개 찾기\n",
    "        sorted_misclassifications = sorted(per_class_misclassifications.items(), key=lambda x: x[1] if x[1] is not None else -1, reverse=True)\n",
    "        worst_classes = sorted_misclassifications[:5]\n",
    "\n",
    "        # 잘못된 예측이 많은 클래스 로그\n",
    "        worst_classes_table = wandb.Table(columns=[\"class_id\", \"misclassifications\"])\n",
    "        for class_id, miscount in worst_classes:\n",
    "            worst_classes_table.add_data(class_id, miscount)\n",
    "        wandb.log({\"worst_classes_misclassifications\": worst_classes_table})\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            'val_loss_epoch': total_loss / len(self.val_loader),\n",
    "            'val_accuracy_epoch': val_accuracy,\n",
    "            'epoch': epoch+1,\n",
    "        })\n",
    "\n",
    "        # Log confusion matrix to wandb\n",
    "        wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                   y_true=true_labels,\n",
    "                                                                   preds=pred_labels,\n",
    "                                                                   class_names=[str(i) for i in range(self.num_classes)])})\n",
    "\n",
    "        return total_loss / len(self.val_loader), val_accuracy\n",
    "\n",
    "\n",
    "    def train(self) -> None:\n",
    "        # 전체 훈련 과정을 관리\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "\n",
    "            train_loss, train_accuracy = self.train_epoch(epoch)\n",
    "            val_loss, val_accuracy = self.validate(epoch)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "            print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\\n\")\n",
    "\n",
    "            self.save_model(epoch, val_loss)\n",
    "\n",
    "            # Early Stopping 조건 확인\n",
    "            self.early_stopping(val_loss)\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered. Stopping training...\")\n",
    "                break\n",
    "\n",
    "            self.scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f41c09-318a-4f2e-bdca-68d8a07e9938",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "698783c4-ac2a-4e66-82aa-637df06ce012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 장비를 선택.\n",
    "# torch라이브러리에서 gpu를 인식할 경우, cuda로 설정.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76cfe17e-fb14-42e4-84ae-b6773f0b78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터의 경로와 정보를 가진 파일의 경로를 설정.\n",
    "traindata_dir = \"/data/ephemeral/home/data/train\"\n",
    "traindata_info_file = \"/data/ephemeral/home/data/train.csv\"\n",
    "save_result_path = \"/data/ephemeral/home/youngtae/model2/Autoaugment_training_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42a4778f-bfd0-4638-8972-f366cd6b0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터의 class, image path, target에 대한 정보가 들어있는 csv파일을 읽기.\n",
    "train_info = pd.read_csv(traindata_info_file)\n",
    "\n",
    "# 총 class의 수를 측정.\n",
    "num_classes = len(train_info['target'].unique())\n",
    "\n",
    "# 각 class별로 8:2의 비율이 되도록 학습과 검증 데이터를 분리.\n",
    "train_df, val_df = train_test_split(\n",
    "    train_info, \n",
    "    test_size=0.2,\n",
    "    stratify=train_info['target'],\n",
    "    random_state=seed  # 랜덤 시드 적용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0889892-5f63-4dcf-bcab-9ff2659ad28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform 설정\n",
    "transform_selector = TransformSelector(\n",
    "    transform_type=\"torchvision\"\n",
    ")\n",
    "train_transform = transform_selector.get_transform(is_train=True)\n",
    "val_transform = transform_selector.get_transform(is_train=False)\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = CustomDataset(\n",
    "    root_dir=traindata_dir,\n",
    "    info_df=train_df,\n",
    "    transform=train_transform\n",
    ")\n",
    "val_dataset = CustomDataset(\n",
    "    root_dir=traindata_dir,\n",
    "    info_df=val_df,\n",
    "    transform=val_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22b2f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader에서 worker_init_fn을 설정하여 각 워커의 시드를 고정\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(seed + worker_id)\n",
    "    random.seed(seed + worker_id)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    worker_init_fn=worker_init_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    worker_init_fn=worker_init_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45885191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/youngtae/wandb/run-20240919_024450-t405cr6b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/youngtae0818-naver/experiment_baseline_2/runs/t405cr6b' target=\"_blank\">neat-star-4</a></strong> to <a href='https://wandb.ai/youngtae0818-naver/experiment_baseline_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/youngtae0818-naver/experiment_baseline_2' target=\"_blank\">https://wandb.ai/youngtae0818-naver/experiment_baseline_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/youngtae0818-naver/experiment_baseline_2/runs/t405cr6b' target=\"_blank\">https://wandb.ai/youngtae0818-naver/experiment_baseline_2/runs/t405cr6b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/youngtae0818-naver/experiment_baseline_2/runs/t405cr6b?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fad46d55fc0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb 초기화\n",
    "wandb.init(project='experiment_baseline_2', config={\n",
    "    'epochs': 50,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.001,\n",
    "    'optimizer': 'AdamW',\n",
    "    'model': 'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k',\n",
    "    'transform': 'AutoAugment',\n",
    "    'seed': seed,\n",
    "    'num_classes': num_classes\n",
    "    # Add other hyperparameters here\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9447ca16-a1ff-4da4-b5a1-4cf239ebcf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 Model을 선언.\n",
    "model_selector = ModelSelector(\n",
    "    model_type='timm', \n",
    "    num_classes=num_classes,\n",
    "    model_name='eva02_large_patch14_448.mim_m38m_ft_in22k_in1k', \n",
    "    pretrained=True\n",
    ")\n",
    "model = model_selector.get_model()\n",
    "\n",
    "# 선언된 모델을 학습에 사용할 장비로 셋팅.\n",
    "model.to(device)\n",
    "\n",
    "# 모델의 모든 파라미터를 동결 (학습되지 않도록 설정)\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# block.22, block.23과 head 레이어만 학습되도록 설정\n",
    "for name, param in model.named_parameters():\n",
    "    if 'blocks.23' in name or 'head' in name:\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c32f36d-4031-4cf4-8914-6e01d8861837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=wandb.config.learning_rate  # wandb.config에서 학습률 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2bd831e-39a7-4f10-a3a5-aefde280fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케줄러 초기화\n",
    "scheduler_step_size = 30  # 매 30step마다 학습률 감소\n",
    "scheduler_gamma = 0.1  # 학습률을 현재의 10%로 감소\n",
    "\n",
    "# 한 epoch당 step 수 계산\n",
    "steps_per_epoch = len(train_loader)\n",
    "\n",
    "# 2 epoch마다 학습률을 감소시키는 스케줄러 선언\n",
    "epochs_per_lr_decay = 2\n",
    "scheduler_step_size = steps_per_epoch * epochs_per_lr_decay\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    step_size=scheduler_step_size, \n",
    "    gamma=scheduler_gamma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32715cca-5a4a-49d5-8fd9-f84da4581523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 Loss를 선언.\n",
    "loss_fn = Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d17e2556-efcf-42c6-9d73-dc27813e7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 선언한 필요 class와 변수들을 조합해, 학습을 진행할 Trainer를 선언.\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    device=device, \n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader, \n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_fn=loss_fn, \n",
    "    epochs=wandb.config.epochs,\n",
    "    result_path=save_result_path,\n",
    "    num_classes=num_classes  # 추가됨\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b87d11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Truncating wandb.Table object to 200000 rows.             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.1720, Train Accuracy: 77.62%\n",
      "Epoch 1, Validation Loss: 0.4688, Validation Accuracy: 87.35%\n",
      "\n",
      "Save 0epoch result. Loss = 0.4688\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Truncating wandb.Table object to 200000 rows.             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.3335, Train Accuracy: 90.67%\n",
      "Epoch 2, Validation Loss: 0.4129, Validation Accuracy: 88.45%\n",
      "\n",
      "Save 1epoch result. Loss = 0.4129\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Truncating wandb.Table object to 200000 rows.              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.2425, Train Accuracy: 93.03%\n",
      "Epoch 3, Validation Loss: 0.3845, Validation Accuracy: 89.92%\n",
      "\n",
      "Save 2epoch result. Loss = 0.3845\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Truncating wandb.Table object to 200000 rows.              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.1753, Train Accuracy: 94.62%\n",
      "Epoch 4, Validation Loss: 0.4428, Validation Accuracy: 89.88%\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Truncating wandb.Table object to 200000 rows.              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.1598, Train Accuracy: 95.31%\n",
      "Epoch 5, Validation Loss: 0.4429, Validation Accuracy: 89.52%\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Truncating wandb.Table object to 200000 rows.              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.1428, Train Accuracy: 95.91%\n",
      "Epoch 6, Validation Loss: 0.4095, Validation Accuracy: 90.55%\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Truncating wandb.Table object to 200000 rows.               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.1141, Train Accuracy: 96.63%\n",
      "Epoch 7, Validation Loss: 0.4395, Validation Accuracy: 90.22%\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Truncating wandb.Table object to 200000 rows.               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.1154, Train Accuracy: 96.48%\n",
      "Epoch 8, Validation Loss: 0.4396, Validation Accuracy: 90.28%\n",
      "\n",
      "Early stopping triggered. Stopping training...\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c19dfa88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>global_step</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇█████</td></tr><tr><td>train_accuracy_batch</td><td>▁▅▅▅▆▆▇▇▇▇▇▇▇▆█▆▇▇█▇▇█▇▇██▇▇▇▇█▇███▇▇█▇▇</td></tr><tr><td>train_accuracy_epoch</td><td>▁▆▇▇████</td></tr><tr><td>train_loss_batch</td><td>█▆▂▂▁▁▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy_epoch</td><td>▁▃▇▇▆█▇▇</td></tr><tr><td>val_loss_batch</td><td>▅█▄▆▂▃▆▆▆▃▃▅▅▅▄▃▇▄▃▅▃▅▅▅▂▂▆▄▄▂█▃▄▁▆▇▆▆▂▄</td></tr><tr><td>val_loss_epoch</td><td>█▃▁▆▆▃▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>8</td></tr><tr><td>global_step</td><td>1504</td></tr><tr><td>train_accuracy_batch</td><td>95.83333</td></tr><tr><td>train_accuracy_epoch</td><td>96.47969</td></tr><tr><td>train_loss_batch</td><td>0.08517</td></tr><tr><td>train_loss_epoch</td><td>0.11539</td></tr><tr><td>val_accuracy_epoch</td><td>90.28286</td></tr><tr><td>val_loss_batch</td><td>0.58971</td></tr><tr><td>val_loss_epoch</td><td>0.43964</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neat-star-4</strong> at: <a href='https://wandb.ai/youngtae0818-naver/experiment_baseline_2/runs/t405cr6b' target=\"_blank\">https://wandb.ai/youngtae0818-naver/experiment_baseline_2/runs/t405cr6b</a><br/> View project at: <a href='https://wandb.ai/youngtae0818-naver/experiment_baseline_2' target=\"_blank\">https://wandb.ai/youngtae0818-naver/experiment_baseline_2</a><br/>Synced 5 W&B file(s), 0 media file(s), 32 artifact file(s) and 176 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240919_024450-t405cr6b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb run 종료\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11087088-9b1f-4f7d-8eb5-72008cc88a50",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bf92c3c-7b38-4f89-af2a-5dfbabf51926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 추론을 위한 함수\n",
    "def inference(\n",
    "    model: nn.Module, \n",
    "    device: torch.device, \n",
    "    test_loader: DataLoader\n",
    "):\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad():  # Gradient 계산을 비활성화\n",
    "        for images in tqdm(test_loader):\n",
    "            # 데이터를 같은 장치로 이동\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # 모델을 통해 예측 수행\n",
    "            logits = model(images)\n",
    "            logits = F.softmax(logits, dim=1)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            # 예측 결과 저장\n",
    "            predictions.extend(preds.cpu().detach().numpy())  # 결과를 CPU로 옮기고 리스트에 추가\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b407c24c-785d-4ffc-b17b-84ae7dc4ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 데이터의 경로와 정보를 가진 파일의 경로를 설정.\n",
    "testdata_dir = \"/data/ephemeral/home/data/test\"\n",
    "testdata_info_file = \"/data/ephemeral/home/data/test.csv\"\n",
    "save_result_path = \"/data/ephemeral/home/youngtae/model2/Autoaugment_training_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbb89c12-3b5d-4647-a8c2-83650dce6281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 데이터의 class, image path, target에 대한 정보가 들어있는 csv파일을 읽기.\n",
    "test_info = pd.read_csv(testdata_info_file)\n",
    "\n",
    "# 총 class 수.\n",
    "num_classes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecec8773-6045-401e-b307-0a9758374c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론에 사용할 Transform을 선언.\n",
    "transform_selector = TransformSelector(\n",
    "    transform_type = \"torchvision\"\n",
    ")\n",
    "test_transform = transform_selector.get_transform(is_train=False)\n",
    "\n",
    "# 추론에 사용할 Dataset을 선언.\n",
    "test_dataset = CustomDataset(\n",
    "    root_dir=testdata_dir,\n",
    "    info_df=test_info,\n",
    "    transform=test_transform,\n",
    "    is_inference=True\n",
    ")\n",
    "\n",
    "# 추론에 사용할 DataLoader를 선언.\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    worker_init_fn=worker_init_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99cc06c1-ce65-476b-8d5b-b8025fcde443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론에 사용할 장비를 선택.\n",
    "# torch라이브러리에서 gpu를 인식할 경우, cuda로 설정.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 추론에 사용할 Model을 선언.\n",
    "model_selector = ModelSelector(\n",
    "    model_type='timm', \n",
    "    num_classes=num_classes,\n",
    "    model_name='eva02_large_patch14_448.mim_m38m_ft_in22k_in1k', \n",
    "    pretrained=False\n",
    ")\n",
    "model = model_selector.get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "317b966a-254c-4ac6-b9b4-1d39f59d524a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best epoch 모델을 불러오기.\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        os.path.join(save_result_path, \"best_model.pt\"),\n",
    "        map_location=device\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "514852af-f338-4b27-a5a5-8b65406a8023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [10:32<00:00,  4.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# predictions를 CSV에 저장할 때 형식을 맞춰서 저장\n",
    "# 테스트 함수 호출\n",
    "predictions = inference(\n",
    "    model=model, \n",
    "    device=device, \n",
    "    test_loader=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc96c889-2423-42b2-8c3c-4b1d364ece71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>image_path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.JPEG</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.JPEG</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.JPEG</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.JPEG</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.JPEG</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>10009</td>\n",
       "      <td>10009.JPEG</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>10010</td>\n",
       "      <td>10010.JPEG</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>10011</td>\n",
       "      <td>10011.JPEG</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>10012</td>\n",
       "      <td>10012.JPEG</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>10013</td>\n",
       "      <td>10013.JPEG</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10014 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  image_path  target\n",
       "0          0      0.JPEG     328\n",
       "1          1      1.JPEG     414\n",
       "2          2      2.JPEG     493\n",
       "3          3      3.JPEG      17\n",
       "4          4      4.JPEG     388\n",
       "...      ...         ...     ...\n",
       "10009  10009  10009.JPEG     235\n",
       "10010  10010  10010.JPEG     191\n",
       "10011  10011  10011.JPEG     466\n",
       "10012  10012  10012.JPEG     258\n",
       "10013  10013  10013.JPEG     210\n",
       "\n",
       "[10014 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 클래스에 대한 예측 결과를 하나의 문자열로 합침\n",
    "test_info['target'] = predictions\n",
    "test_info = test_info.reset_index().rename(columns={\"index\": \"ID\"})\n",
    "test_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4efd2f6-d74a-491b-a7b1-fd7cf96f45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 저장\n",
    "test_info.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79e3df-e5c3-4a49-b37e-0dea1300c317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
